---
phase: 15-seo-fundamentals
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/web/src/app/sitemap.ts
  - apps/web/src/app/robots.ts
autonomous: true

must_haves:
  truths:
    - "/sitemap.xml returns valid XML with all public page URLs"
    - "/robots.txt allows crawling of public pages"
    - "/robots.txt blocks /dashboard, /admin, /api, /settings"
    - "Sitemap includes homepage, pricing, privacy, terms, login, signup pages"
  artifacts:
    - path: "apps/web/src/app/sitemap.ts"
      provides: "Dynamic sitemap generation"
      exports: ["default"]
    - path: "apps/web/src/app/robots.ts"
      provides: "Robots.txt rules"
      exports: ["default"]
  key_links:
    - from: "apps/web/src/app/robots.ts"
      to: "/sitemap.xml"
      via: "sitemap property"
      pattern: "sitemap.*sitemap\\.xml"
---

<objective>
Create sitemap.xml and robots.txt using Next.js native file conventions.

Purpose: Enable search engine discovery of public marketing pages while blocking authenticated routes from crawling.

Output: Two new files (sitemap.ts, robots.ts) that generate /sitemap.xml and /robots.txt at build time.
</objective>

<execution_context>
@C:\Users\aaron\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\aaron\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-seo-fundamentals/15-CONTEXT.md
@.planning/phases/15-seo-fundamentals/15-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sitemap.ts</name>
  <files>apps/web/src/app/sitemap.ts</files>
  <action>
Create apps/web/src/app/sitemap.ts that returns MetadataRoute.Sitemap array.

Include these public pages with appropriate priority and changeFrequency:
- / (priority: 1.0, changeFrequency: 'weekly')
- /pricing (priority: 0.8, changeFrequency: 'monthly')
- /privacy (priority: 0.3, changeFrequency: 'yearly')
- /terms (priority: 0.3, changeFrequency: 'yearly')
- /login (priority: 0.5, changeFrequency: 'monthly')
- /signup (priority: 0.8, changeFrequency: 'monthly')
- /demo (priority: 0.7, changeFrequency: 'monthly')

Use BASE_URL from process.env.NEXT_PUBLIC_APP_URL or default to 'https://peacase.com'.

Set lastModified to new Date() for all pages (dynamic generation).

Import MetadataRoute from 'next' for proper TypeScript types.
  </action>
  <verify>
1. File exists at apps/web/src/app/sitemap.ts
2. TypeScript compiles: cd apps/web && npx tsc --noEmit src/app/sitemap.ts
3. Run dev server and curl http://localhost:3000/sitemap.xml returns valid XML
  </verify>
  <done>
sitemap.ts exports default function returning MetadataRoute.Sitemap array with 7 public page entries
  </done>
</task>

<task type="auto">
  <name>Task 2: Create robots.ts</name>
  <files>apps/web/src/app/robots.ts</files>
  <action>
Create apps/web/src/app/robots.ts that returns MetadataRoute.Robots object.

Configure rules:
- userAgent: '*'
- allow: '/'
- disallow: ['/dashboard', '/dashboard/*', '/admin', '/admin/*', '/api', '/api/*', '/settings', '/settings/*', '/staff', '/staff/*', '/calendar', '/clients', '/services', '/packages', '/gift-cards', '/marketing', '/reports', '/notifications', '/locations', '/onboarding', '/setup', '/portal', '/embed']

Include sitemap reference: `${BASE_URL}/sitemap.xml` where BASE_URL is process.env.NEXT_PUBLIC_APP_URL or 'https://peacase.com'.

Import MetadataRoute from 'next' for proper TypeScript types.
  </action>
  <verify>
1. File exists at apps/web/src/app/robots.ts
2. TypeScript compiles: cd apps/web && npx tsc --noEmit src/app/robots.ts
3. Run dev server and curl http://localhost:3000/robots.txt returns valid robots.txt with Sitemap line
  </verify>
  <done>
robots.ts exports default function returning MetadataRoute.Robots with allow /, comprehensive disallow list, and sitemap reference
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Start dev server: cd apps/web && npm run dev
2. Verify sitemap: curl http://localhost:3000/sitemap.xml
   - Returns XML with urlset element
   - Contains 7 URLs (/, /pricing, /privacy, /terms, /login, /signup, /demo)
   - Each URL has loc, lastmod, changefreq, priority
3. Verify robots: curl http://localhost:3000/robots.txt
   - Contains "User-agent: *"
   - Contains "Allow: /"
   - Contains "Disallow: /dashboard"
   - Contains "Sitemap: https://peacase.com/sitemap.xml"
4. TypeScript check: cd apps/web && npx tsc --noEmit
</verification>

<success_criteria>
- /sitemap.xml returns valid XML with all 7 public page URLs
- /robots.txt allows root crawling and blocks authenticated routes
- Both files use proper Next.js MetadataRoute types
- No TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/15-seo-fundamentals/15-01-SUMMARY.md`
</output>
